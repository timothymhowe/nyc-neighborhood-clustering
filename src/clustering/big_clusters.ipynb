{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import cupy as np\n",
    "import cupyx.scipy as scipy\n",
    "import h3pandas\n",
    "import random\n",
    "\n",
    "# from dask.distributed import Client\n",
    "\n",
    "# cluster = LocalCUDACluster()\n",
    "# client = Client(cluster)\n",
    "import sklearn.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nypd_data = pd.read_csv('../../data/clean/police_data/NYPDcleanupfinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def plot_elbow(kmean, X):\n",
    "#     centroids = [k.cluster_centers_ for k in kmean]\n",
    "#     D_k = [cdist(X, center, 'euclidean') for center in centroids]\n",
    "#     dist = [np.min(D, axis=1) for D in D_k]\n",
    "#\n",
    "#     # Total with-in sum of square\n",
    "#     wcss = [sum(d ** 2) for d in dist]\n",
    "#     tss = sum(pdist(X) ** 2) / X.shape[0]\n",
    "#     bss = tss - wcss\n",
    "#\n",
    "#     plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n",
    "#     ax = plt.subplot(1, 1, 1)\n",
    "#     ax.plot(Ks, bss / tss * 100, 'b*-')\n",
    "#     plt.grid(True)\n",
    "#     plt.xlabel('Number of clusters')\n",
    "#     plt.ylabel('Percentage of variance explained (%)')\n",
    "#     plt.title('Elbow for KMeans clustering')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sales_data = pd.read_csv('../../data/clean/SalesFinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "building_data = gpd.read_file('../../data/clean/building_stats.csv')\n",
    "park_mask = gpd.read_file('../../data/clean/park_hexes.csv')\n",
    "building_data = building_data.set_index('hex_id')\n",
    "park_mask = park_mask.set_index('hex_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hex_files = ('bronx_hex.csv', 'brooklyn_hex.csv', 'manhattan_hex.csv', 'queens_hex.csv', 'staten_hex.csv')\n",
    "hex_dfs = []\n",
    "for f in hex_files:\n",
    "    hex_dfs = hex_dfs + list(pd.read_csv(f\"../../data/clean/h3_index/{f}\")[f.split('.')[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hex_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nypd_data = nypd_data[nypd_data['h3geo'].isin(hex_dfs)]\n",
    "nypd_data['RPT_DT'] = pd.to_datetime(nypd_data['RPT_DT'])\n",
    "# nypd_data = nypd_data[nypd_data['RPT_DT'].dt.year == 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sales_data = sales_data[sales_data['h3geo'].isin(hex_dfs)]\n",
    "sales_data['BUILDING_CLASS'] = sales_data['BUILDING CLASS AT TIME OF SALE']\n",
    "\n",
    "selected_cols_sales = sales_data[[\n",
    "    'h3geo',\n",
    "    'BUILDING_CLASS',\n",
    "    'YEAR BUILT',\n",
    "    'SALE PRICE'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clean_nypd = nypd_data.set_index('h3geo')\n",
    "clean_nypd['lat'] = clean_nypd['Latitude']\n",
    "clean_nypd['lon'] = clean_nypd['Longitude']\n",
    "clean_nypd = clean_nypd.drop(['Latitude', 'Longitude'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "selected_cols_nypd = clean_nypd[[\n",
    "    'CMPLNT_FR_TM',\n",
    "    'RPT_DT',\n",
    "    'KY_CD',\n",
    "    'OFNS_DESC',\n",
    "    'CRM_ATPT_CPTD_CD',\n",
    "    'LAW_CAT_CD',\n",
    "    'JURIS_DESC',\n",
    "    ]]\n",
    "selected_cols_nypd = selected_cols_nypd[selected_cols_nypd['OFNS_DESC'].isnull() == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_hour_of_day(thetime):\n",
    "    strips = thetime.split(':')\n",
    "    hour = int(strips[0])\n",
    "    if hour < 5:\n",
    "        return 'NIGHT'\n",
    "    elif hour < 11:\n",
    "        return 'MORNING'\n",
    "    elif hour < 17:\n",
    "        return 'DAY'\n",
    "    elif hour < 23:\n",
    "        return 'EVENING'\n",
    "    else:\n",
    "        return 'NIGHT'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# selected_cols_nypd['FUZZY_TIME'] = selected_cols_nypd['CMPLNT_FR_TM'].apply(get_hour_of_day)\n",
    "selected_cols_nypd = selected_cols_nypd.drop(['CMPLNT_FR_TM', 'RPT_DT'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nypd_fe = pd.get_dummies(selected_cols_nypd,\n",
    "                         # columns=['OFNS_DESC', 'CRM_ATPT_CPTD_CD', 'LAW_CAT_CD', 'JURIS_DESC', 'KY_CD', 'FUZZY_TIME'],\n",
    "                         columns=['OFNS_DESC', 'CRM_ATPT_CPTD_CD', 'LAW_CAT_CD', 'JURIS_DESC', 'KY_CD'],\n",
    "                         prefix_sep='_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nypd_fe = nypd_fe.groupby('h3geo').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "selected_cols_sales['BUILDING_CLASS'] = selected_cols_sales['BUILDING_CLASS'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sales_fe = pd.get_dummies(selected_cols_sales, columns=['BUILDING_CLASS']).set_index('h3geo')\n",
    "sales_fe['COUNT'] = 1\n",
    "sales_fe2 = sales_fe.groupby('h3geo').sum()\n",
    "sales_fe = sales_fe.groupby('h3geo').sum()\n",
    "sales_fe['MEAN_BLD_AGE'] = 2023 - (sales_fe['YEAR BUILT'] / sales_fe['COUNT'])\n",
    "sales_fe['MEAN_SALE_PRICE'] = sales_fe['SALE PRICE'] / sales_fe['COUNT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sales_fe3 = sales_fe2.div(sales_fe2['COUNT'], axis=0)\n",
    "sales_fe2 = sales_fe2.drop(['YEAR BUILT', 'SALE PRICE', 'COUNT'], axis=1)\n",
    "\n",
    "# sales_fe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sales_fe = sales_fe.drop(['YEAR BUILT', 'SALE PRICE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sales_fe = sales_fe[['COUNT', 'MEAN_BLD_AGE', 'MEAN_SALE_PRICE']]\n",
    "sales_fe = sales_fe.join(sales_fe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "helper_outer = sales_fe.join(nypd_fe, how='outer').join(building_data, how='outer')\n",
    "helper_inner = sales_fe.join(nypd_fe, how='inner')\n",
    "helper_inner = nypd_fe\n",
    "\n",
    "wopt = pd.DataFrame(hex_dfs, columns=['hex_id'])\n",
    "wopt = wopt.set_index('hex_id')\n",
    "all_hexes = wopt.join(helper_outer, how='inner')\n",
    "all_hexes.shape\n",
    "wopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "helper_all = pd.DataFrame(hex_dfs, columns=['h3geo']).set_index('h3geo')\n",
    "police_all = helper_all.join(nypd_fe, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "helper = pd.read_csv('../../data/clean/h3_index/brooklyn_hex.csv')\n",
    "helper.columns = ['h3geo']\n",
    "helper = helper.set_index('h3geo')\n",
    "# helper = helper.join(sales_fe, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# helper = helper.join(nypd_fe, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# helper_outer = helper_outer_nonan ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from libpysal.weights import Queen, KNN\n",
    "from esda.moran import Moran\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cluster_variables = ['BldgArea', 'YearBuilt', 'AssessTot', 'ResArea'\n",
    "\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# #1.1 data preparation\n",
    "# X = helper_inner.iloc[:, 1:].values\n",
    "# # Using the elbow method to find the optimal number of clusters\n",
    "# wcss = []\n",
    "# for i in range(1, 60):\n",
    "#     kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "#     kmeans.fit(X)\n",
    "#     wcss.append(kmeans.inertia_)\n",
    "# plt.plot(range(1, 60), wcss)\n",
    "# plt.title('The Elbow Method')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('WCSS')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_map = helper_all.join(helper_inner, how='left').fillna(method='ffill')\n",
    "# df_map = helper.join(helper_inner, how='inner')\n",
    "# df_map = helper_inner\n",
    "w = Queen.from_dataframe(df_map.h3.h3_to_geo_boundary().reset_index())\n",
    "\n",
    "clean = df_map.h3.h3_to_geo_boundary().reset_index().drop(w.islands)\n",
    "w2 = Queen.from_dataframe(clean)\n",
    "w = w2\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import robust_scale\n",
    "\n",
    "# db_scaled = robust_scale(clean.set_index('h3geo').drop(['COUNT', 'geometry'], axis=1))\n",
    "db_scaled = robust_scale(clean.set_index('h3geo').drop(['geometry'], axis=1))\n",
    "\n",
    "# # Set the seed for reproducibility\n",
    "# np.random.seed(123456)\n",
    "# # Specify cluster model with spatial constraint\n",
    "# model = AgglomerativeClustering(\n",
    "#     linkage='average', connectivity=w.sparse, n_clusters=5, compute_full_tree=True\n",
    "#     )\n",
    "# # Fit algorithm to the data\n",
    "# model.fit(db_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clean['ward5wq'] = model.labels_\n",
    "# Setup figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(30, 30))\n",
    "# Plot unique values choropleth including a legend and with no boundary lines\n",
    "clean.plot(column='ward5wq', legend=True, linewidth=0, ax=ax)\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clean[, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db_scaled = robust_scale(all_hexes)\n",
    "all_hexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.5, min_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db_p = db.fit_predict(db_scaled)\n",
    "helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dew_it = pd.DataFrame(db_p).set_index(police_all.index)\n",
    "dew_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db_scaled_copy = db_scaled.copy()\n",
    "db_scaled_copy = pd.DataFrame(db_scaled_copy)\n",
    "db_scaled_copy = db_scaled_copy.set_index(police_all.index)\n",
    "db_scaled_copy.columns = police_all.columns\n",
    "db_scaled_copy['Cluster'] = db_p\n",
    "# final = db_scaled_copy.groupby('Cluster').mean()\n",
    "# # final = final.transpose().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db_scaled_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db_scaled_copy = db_scaled_copy.h3.h3_to_geo_boundary()\n",
    "db_scaled_copy.index.names = ['hex_id']\n",
    "\n",
    "db_scaled_copy_2 = mask(db_scaled_copy.reset_index(), the_mask)\n",
    "# Setup figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(30, 30))\n",
    "# Plot unique values choropleth including a legend and with no boundary lines\n",
    "db_scaled_copy_2.plot(column='Cluster', categorical=True, legend=True, linewidth=0, ax=ax)\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "_CRS = 'espg:4326'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# subway_map = pd.read_csv('../../data/raw/subway_lines.csv')\n",
    "# subway_map['geometry'] = subway_map['the_geom']\n",
    "# subway_map = gpd.GeoDataFrame(subway_map,crs=_CRS,geometry='the_geom')\n",
    "# f, ax = plt.subplots(1, figsize=(10, 40))\n",
    "# # Plot unique values choropleth including a legend and with no boundary lines\n",
    "# subway_map.plot(column='NAME', categorical=True, legend=False, linewidth=0.5, ax=ax)\n",
    "# # Remove axis\n",
    "# ax.set_axis_off();\n",
    "# Display the map\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PCA(db_scaled).score()\n",
    "db_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neighborhood_centroids = pd.read_csv('../../data/clean/neighborhood_centroids.csv')\n",
    "from h3 import h3_distance\n",
    "import h3\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics.pairwise import euclidean_distances, nan_euclidean_distances\n",
    "import random, os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class CellFactory:\n",
    "\n",
    "    def __init__(self, hex_list, data_list, neighborhood_id_list, output_dir_name):\n",
    "        \"\"\" Initializes new object\n",
    "        :param hex_list:\n",
    "        :param data_list:\n",
    "        :param neighborhood_id_list:\n",
    "        \"\"\"\n",
    "        now = datetime.now()\n",
    "        self.output_dir_name = f\"{output_dir_name}_{now.strftime('%D-%H-%M-%S')}\"\n",
    "        self.results_df_list = []\n",
    "\n",
    "        self.hex_list = hex_list\n",
    "        self.data_list = data_list\n",
    "        self.neighborhood_id_list = neighborhood_id_list.set_index('hex_id')\n",
    "\n",
    "        self.neighborhood_id_list = self.neighborhood_id_list.join(self.hex_list, how='left')\n",
    "        # assert sum([~self.neighborhood_id_list['hex_id'].isin(self.hex_list)]) == 0\n",
    "\n",
    "        self.cell_dict = {}\n",
    "        self.neighborhood_dict = {}\n",
    "\n",
    "    def build_cell_dict(self, all_cells=False):\n",
    "        if all_cells:\n",
    "            index_list = self.hex_list.join(self.data_list, how=\"left\", lsuffix='L')\n",
    "        else:\n",
    "            index_list = self.hex_list.join(self.data_list, how=\"inner\", lsuffix='L')\n",
    "\n",
    "        # index_list = self.hex_list.join(self.data_list, how=\"inner\", lsuffix='L')\n",
    "        # self.neighborhood_id_list = self.neighborhood_id_list.filter(items=index_list['hex_id'], axis=0).filter(i)\n",
    "\n",
    "        for index, row in index_list.iterrows():\n",
    "            hex_id = row['hex_id']\n",
    "            #TODO: DON\"T HARD CODE THIS, ITS BAD.  Set the correct pre and post conditions for the data dfs!!\n",
    "            # TODO: STILL GOTTA FIX THIS, DOOFUS\n",
    "            self.cell_dict[hex_id] = HexCell(hex_id, self, data_values=list(row[2:]))\n",
    "\n",
    "        self.trim_neighbors()\n",
    "\n",
    "    def generate_neighborhoods(self):\n",
    "        self.neighborhood_dict = {}\n",
    "        for hood_id, hood_name in self.neighborhood_id_list.iterrows():\n",
    "            self.neighborhood_dict[hood_id] = GreedyNeighborhood(self.cell_dict[hood_id], hood_name[0], factory=self)\n",
    "\n",
    "    def get_cell_dict(self):\n",
    "        return self.cell_dict\n",
    "\n",
    "    def trim_neighbors(self):\n",
    "        \"\"\" Void method that trims cell neighbors\n",
    "        \\n :Pre Cell Dict has been populated with list of all viable cells, and each cell has had their neighbor lists populated.\n",
    "        \\n :Post each cell in self.cell_dict only has neighbors who are also in the cell dict.\n",
    "        \"\"\"\n",
    "        for key in self.cell_dict.keys():\n",
    "            self.cell_dict[key].neighbor_ids = [the_id for the_id in self.cell_dict[key].neighbor_ids if\n",
    "                                                the_id in self.cell_dict.keys()]\n",
    "            self.cell_dict[key].wanted_cells = [the_id for the_id in self.cell_dict[key].neighbor_ids if\n",
    "                                                the_id in self.cell_dict.keys()]\n",
    "\n",
    "    def simulate(self, n=2, graph_during=False):\n",
    "        # For the number of rounds specified by the user:\n",
    "        for i in range(1, n + 1):\n",
    "            # For each neighborhood in the list\n",
    "            for neighborhood_id in self.neighborhood_dict.keys():\n",
    "                neighborhood = self.neighborhood_dict[neighborhood_id]\n",
    "                print(f\"Simulating round {i} of {n}\")\n",
    "                print(f\"Current Neighborhood: {neighborhood.get_name()}\")\n",
    "                if neighborhood.cell_count:\n",
    "                    neighborhood.play_turn()\n",
    "                clear_output(wait=True)\n",
    "                self.neighborhood_dict[neighborhood_id] = neighborhood.update_factory()\n",
    "\n",
    "            self.results_df_list = self.results_df_list + [self.get_results()]\n",
    "            if graph_during:\n",
    "                self.plot_neighborhoods(i)\n",
    "\n",
    "    def get_results(self):\n",
    "        results_list = []\n",
    "        for hood in self.neighborhood_list:\n",
    "            # print(hood.output_cell_map())\n",
    "            results_list = results_list + [hood.output_cell_map()]\n",
    "\n",
    "        output = pd.concat(results_list)\n",
    "        return output\n",
    "\n",
    "    def plot_neighborhoods(self, iteration):\n",
    "        results = self.get_results()\n",
    "        # results = results[~results['hex_id'].isin(list(park_mask.reset_index()['hex_id']))]\n",
    "        results = results.set_index('hex_id')\n",
    "        results_plot = df.h3.h3_to_geo_boundary()\n",
    "\n",
    "        # Setup figure and ax\n",
    "        f, ax = plt.subplots(1, figsize=(10, 40))\n",
    "        # Plot unique values choropleth including a legend and with no boundary lines\n",
    "        results_plot.plot(column='name', categorical=True, legend=False, linewidth=0.5, ax=ax)\n",
    "        # Remove axis\n",
    "        ax.set_axis_off()\n",
    "        # Display the map\n",
    "        plt.show()\n",
    "        fstring = f'../../out/images/plt/{self.output_dir_name}'\n",
    "        os.mkdir(fstring)\n",
    "        plt.savefig(f\"{fstring}--{iteration}\")\n",
    "\n",
    "    def do_it_myself(self):\n",
    "        df = pd.DataFrame()\n",
    "        for key, cell in self.cell_dict.items():\n",
    "            df[key] = cell.neighborhood_id\n",
    "\n",
    "        return df\n",
    "\n",
    "    # determine which\n",
    "    # this would be the point to inject the annealing step, making subomptimal decisions probabilistically.\n",
    "    def arbitrate_cell_flip(self, requested_hex, new_neighborhood_id, new_affinity, temperature=1):\n",
    "        cell = self.cell_dict[requested_hex]\n",
    "        assert cell.neighborhood_id != new_neighborhood_id\n",
    "\n",
    "        # if the cell is unowned, allow the flip\n",
    "        if cell.neighborhood_id == -1:\n",
    "            cell.neighborhood_id = new_neighborhood_id\n",
    "            self.cell_dict[requested_hex] = cell\n",
    "        else:\n",
    "            current_affinity = cell.affinity\n",
    "            rand = random.uniform(0, 1)\n",
    "\n",
    "            if new_affinity > current_affinity:\n",
    "                if rand < temperature:\n",
    "                    cell_dict[requested_hex].neighborhood_id = new_neighborhood_id\n",
    "                    self.neighborhood_dict[cell.neighborhood_id] = self.neighborhood_dict[\n",
    "                        cell.neighborhood_id].update_edges_after_losing_cell(requested_hex)\n",
    "            else:\n",
    "                if rand > temperature:\n",
    "                    cell_dict[requested_hex].neighborhood_id = new_neighborhood_id\n",
    "                    self.neighborhood_dict[cell.neighborhood_id] = self.neighborhood_dict[\n",
    "                        cell.neighborhood_id].update_edges_after_losing_cell(requested_hex)\n",
    "\n",
    "        current_neighborhood_id = cell.neighborhood_id\n",
    "\n",
    "\n",
    "class CellFactoryRandomStart(CellFactory):\n",
    "\n",
    "    def __init__(self, hex_list, data_list, number_of_clusters, ):\n",
    "        tuple_list = self.load_hexes(hex_list)\n",
    "        for a_tuple in tuple_list:\n",
    "            hexes, proportion = a_tuple\n",
    "            sample_size = round(proportion * number_of_clusters)\n",
    "            selected = random.sample(hexes, sample_size)\n",
    "\n",
    "    def load_hexes(self, hex_list):\n",
    "        hex_files = ('bronx_hex.csv', 'brooklyn_hex.csv', 'manhattan_hex.csv', 'queens_hex.csv', 'staten_hex.csv')\n",
    "        hex_tuples = []\n",
    "        sum_hexes = len(hex_list)\n",
    "        for f_name in hex_files:\n",
    "            boro_hex_list = list(pd.read_csv(f\"../../data/clean/h3_index/{f_name}\")[f_name.split('.')[0]])\n",
    "            boro_hex_list = boro_hex_list[boro_hex_list in hex_list]\n",
    "            boro_size = len(boro_hex_list)\n",
    "            hex_tuples = hex_tuples + [tuple(boro_hex_list, boro_size / sum_hexes)]\n",
    "\n",
    "        return hex_tuples\n",
    "\n",
    "\n",
    "CellFactoryRandomStart(in_hex, db_scaled, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HexCell:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hex_id, factory, data_values=([None] * cell_len)):\n",
    "        self.hex_id = hex_id\n",
    "        self.neighbor_ids = list(h3.k_ring(hex_id))\n",
    "        self.data_values = data_values\n",
    "        self.affinity = -1\n",
    "        self.neighborhood_id = -1\n",
    "        self.oob = True\n",
    "        self.factory = factory\n",
    "\n",
    "        # border score for surface tension metrics\n",
    "        self.border_score = 0\n",
    "\n",
    "    def set_data_values(self, data_values, start_index=0, end_index=cell_len):\n",
    "        self.data_values = data_values\n",
    "\n",
    "    def get_data_values(self, fill_nan=True, zeroes=False):\n",
    "        \"\"\"\n",
    "        Getter method for the data values of the cell.  Can also be used to\n",
    "        :param fill_nan:\n",
    "        :param zeroes:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if fill_nan:\n",
    "            newvals = []\n",
    "            for val in self.data_values:\n",
    "                if pd.isna(val):\n",
    "                    newvals = newvals + [random.uniform(0, 1)]\n",
    "                else:\n",
    "                    newvals = newvals + [val]\n",
    "            return newvals\n",
    "        else:\n",
    "            return self.data_values\n",
    "\n",
    "    def detect_border(self):\n",
    "        for id in self.neighbor_ids:\n",
    "            if self.factory.cell_dict[id].neighborhood_id != self.neighborhood_id:\n",
    "                return True\n",
    "\n",
    "        # for id in self.neighbor_ids:\n",
    "        #           if id in self.factory.cell_dict.keys():\n",
    "        #               if self.factory.cell_dict[id].neighborhood_id != self.neighborhood_id:\n",
    "        #                   return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "\n",
    "class GreedyNeighborhood:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, starting_cell, name, factory, cell_count=1):\n",
    "\n",
    "        #\n",
    "        self.centroid_cell = copy(starting_cell)\n",
    "        self.centroid_id = self.centroid_cell.hex_id\n",
    "        self.centroid_cell.neighborhood_id = self.centroid_id\n",
    "\n",
    "        # store neighborhood name\n",
    "        self.name = name\n",
    "\n",
    "        # initialize neighborhood mean as the value of the initial cell.\n",
    "        self.neighborhood_mean = (self.centroid_cell.get_data_values(True))\n",
    "        self.factory = factory\n",
    "        self.factory.cell_dict[self.centroid_id] = self.centroid_cell\n",
    "\n",
    "        ## initializing lists of cells that are part of the neighborhood\n",
    "        self.border_cells = [self.centroid_id]\n",
    "        self.interior_cells = [self.centroid_id]\n",
    "        self.wanted_cells = []\n",
    "\n",
    "        self.cell_count = cell_count\n",
    "        self.neighborhood_affinity = 1\n",
    "\n",
    "        # print (self.centroid_cell.neighbor_ids)\n",
    "\n",
    "    def play_turn(self):\n",
    "        self.neighborhood_mean = self.recompute_mean()\n",
    "        self.claim_cells()\n",
    "        if len(self.wanted_cells):\n",
    "            self.claim_cells()\n",
    "\n",
    "    def resolve_ownership_disputes(self):\n",
    "        self.add_to_interior([cell for cell in self.get_owned_cells() if\n",
    "                              self.factory.cell_dict[cell].neighborhood_id == self.centroid_id])\n",
    "\n",
    "    # gets a list of all neighbors of bordering cells  and then trims any that are already in the neighborhood be\n",
    "    def update_edges_after_gaining_cell(self, expanded_hex_id):\n",
    "        self.cell_count += 1\n",
    "        self.wanted_cells.remove(expanded_hex_id)\n",
    "        self.border_cells = self.border_cells + [expanded_hex_id]\n",
    "\n",
    "        expanded_cell = self.factory.cell_dict(expanded_hex_id)\n",
    "\n",
    "        # counter for surface tension\n",
    "        border_rating = 0\n",
    "\n",
    "        # for each of the new cell's neighbors\n",
    "        for nb_id in expanded_cell.neighbor_ids:\n",
    "            nb_cell = self.factory.cell_dict[nb_id]\n",
    "            # check if the neighbor is in the current\n",
    "            if nb_cell.neighborhood_id != self.centroid_id:\n",
    "                # if it's not in the current neighborhood and not already in the wanted cells list, add it to the wanted cells list\n",
    "                if nb_id not in self.wanted_cells:\n",
    "                    self.wanted_cells = self.wanted_cells + [nb_id]\n",
    "            else:  # it must be a neighbor, and therefore it must be in the border cells list\n",
    "                border_rating += 1  # increment border rating\n",
    "\n",
    "                # check if all of a border cell's neighbors are in this same neighborhood\n",
    "                has_gaps = pd.Series([nb_cell.neighbor_ids in self.get_owned_cells()]).all()\n",
    "                if ~has_gaps:  # if they are, move it from the border to the body\n",
    "                    self.border_cells = self.border_cells.remove(nb_id)\n",
    "                    self.interior_cells = self.interior_cells + [nb_id]\n",
    "        return self\n",
    "\n",
    "    def update_edges_after_losing_cell(self, lost_hex_id):\n",
    "        self.cell_count -= 1\n",
    "\n",
    "        # update the lists with the lost new cell id\n",
    "        self.wanted_cells = self.wanted_cells + [lost_hex_id]\n",
    "        self.border_cells = self.border_cells.remove(lost_hex_id)\n",
    "\n",
    "        # get lost cell from cell dict\n",
    "        lost_cell = self.factory.cell_dict(lost_hex_id)\n",
    "\n",
    "        # counter for surface tension\n",
    "        border_rating = 0\n",
    "\n",
    "        # # TODO: MAKE THIS CODE WAY MORE PYTHONIC?  LOTS OF FOR LOOPS AND LISTS, COULD PROBABLY SIMPLIFY WITH LIST COMPREHENSION\n",
    "        # for each of the lost cell's neighbors\n",
    "        for nb_id in lost_cell.neighbor_ids:\n",
    "            nb_cell = self.factory.cell_dict[nb_id]\n",
    "            # check if the neighbor is in the current\n",
    "            if nb_cell.neighborhood_id != self.centroid_id:\n",
    "                # if it's not in the current neighborhood and not already in the wanted cells list, add it to the wanted cells list\n",
    "                if nb_id not in self.wanted_cells:\n",
    "                    self.wanted_cells = self.wanted_cells + [nb_id]\n",
    "\n",
    "            else:  # it must be a neighbor, and therefore it must be in the border cells list\n",
    "                border_rating += 1  # increment border rating\n",
    "                # check if all of a border cell's neighbors are in this same neighborhood\n",
    "                has_gaps = pd.Series([nb_cell.neighbor_ids in self.get_owned_cells()]).all()\n",
    "\n",
    "                if ~has_gaps:  # if they are, move it from the interior list to the border list\n",
    "                    self.border_cells = self.border_cells + [nb_id]\n",
    "                    self.interior_cells = self.interior_cells.remove(nb_id)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # def get_wanted_cells(self):\n",
    "    #     for cell_id in self.border_cells:\n",
    "    #         cell = self.factory.cell_dict[cell_id]\n",
    "    #         potentials = cell.neighbor_ids\n",
    "    #         potentials = np.setdiff1d(potentials, self.get_owned_cells())\n",
    "    #         self.wanted_cells = list(self.wanted_cells) + list(potentials)\n",
    "    #         self.wanted_cells = list(set(self.wanted_cells))\n",
    "\n",
    "    def get_owned_cells(self):\n",
    "        return self.border_cells + self.interior_cells\n",
    "\n",
    "    def is_connected(self):\n",
    "        pass\n",
    "\n",
    "    def get_distance(self, X, Y, Y_norm_sqd):\n",
    "        return euclidean_distances(X, Y, Y_norm_squared=Y_norm_sqd, squared=True)\n",
    "\n",
    "    def get_nan_distance(self, X, Y):\n",
    "        return nan_euclidean_distances(X, Y, squared=True)\n",
    "\n",
    "    def claim_cells(self):\n",
    "        assert len(self.wanted_cells) > 0\n",
    "\n",
    "        Y = np.array(self.neighborhood_mean).reshape(1, -1)\n",
    "        X = np.array([self.factory.cell_dict[cell].get_data_values(True) for cell in self.wanted_cells]).reshape(\n",
    "            len(self.wanted_cells), -1)\n",
    "\n",
    "        rez = pd.DataFrame(nan_euclidean_distances(X, Y, squared=True), columns=['dist'])\n",
    "\n",
    "        rez['current_affinity'] = [self.factory.cell_dict[cell].affinity for cell in self.wanted_cells]\n",
    "        rez['delta_affinity'] = rez['dist'] - rez['current_affinity']\n",
    "        rez['id'] = self.wanted_cells\n",
    "\n",
    "        wantMax = True\n",
    "        if wantMax:\n",
    "            # temp = (rez[rez['delta_affinity'] == rez['delta_affinity'].max()])\n",
    "            temp = (rez[rez['dist'] == rez['dist'].max()])\n",
    "        else:\n",
    "            temp = rez[(rez['delta_affinity'] > 0)]\n",
    "\n",
    "        claiming_cells = temp['id']\n",
    "        for _, row in temp.iterrows():\n",
    "            cell_flip_occurred = self.factory.arbitrate_cell_flip(row['id'], self.centroid_id, row['dist'])\n",
    "            if cell_flip_occurred:\n",
    "                self.update_edges_after_gaining_cell(row['id'])\n",
    "            if wantMax:\n",
    "                break\n",
    "\n",
    "            # self.factory.cell_dict[row['id']].affinity = row['dist']\n",
    "            # self.factory.cell_dict[row['id']].neighborhood_id = self.centroid_id\n",
    "\n",
    "        \"\"\"\n",
    "        TODO FIX THIS GARBANZO BEANS\n",
    "        \"\"\"\n",
    "\n",
    "        # self.wanted_cells = np.setdiff1d(self.wanted_cells, claiming_cells)\n",
    "\n",
    "        # self.determine_border()\n",
    "        # self.add_to_interior(claiming_cells)\n",
    "\n",
    "    def determine_border(self):\n",
    "        new_border = []\n",
    "        for cell_id in self.border_cells:\n",
    "            if self.factory.cell_dict[cell_id].neighborhood_id == self.centroid_id:\n",
    "                new_border = new_border + [cell_id]\n",
    "\n",
    "    def add_to_interior(self, new_cell_ids):\n",
    "        check_list = []\n",
    "        for cell_id in new_cell_ids:\n",
    "            check_list = check_list + self.factory.cell_dict[cell_id].neighbor_ids\n",
    "\n",
    "        # check_list  = [x for x in check_list if x in self.factory.cell_dict.keys()]\n",
    "\n",
    "        to_interior_list = []\n",
    "        for cell_id in check_list:\n",
    "            cell = self.factory.cell_dict[cell_id]\n",
    "            if cell.neighborhood_id == self.centroid_id:\n",
    "                if not cell.detect_border():\n",
    "                    to_interior_list = to_interior_list + [cell_id]\n",
    "                else:\n",
    "                    self.border_cells = self.border_cells + [cell_id]\n",
    "\n",
    "        self.interior_cells = list(set(self.interior_cells + to_interior_list))\n",
    "        self.border_cells = list(set(self.border_cells))\n",
    "\n",
    "        # TODO: HANDLE BORDER INCURSIONS\n",
    "\n",
    "    # def a_b_test(self, hex_id):\n",
    "    #\n",
    "    #     self.neighborhood_mean = self.recompute_mean()\n",
    "    #     Y = np.array(self.neighborhood_mean).reshape(1, -1)\n",
    "    #     X = np.array([self.factory.cell_dict[cell].get_data_values(True) for cell in self.wanted_cells]).reshape(\n",
    "    #         len(self.wanted_cells), -1)\n",
    "    #\n",
    "    #     rez = pd.DataFrame(nan_euclidean_distances(X, Y, squared=True), columns=['dist'])\n",
    "\n",
    "    def recompute_mean(self, fill_nan=False, compare=False):\n",
    "        \"\"\"\n",
    "        Recomputes the mean every time for the whole neighborhood.  This sucks, and is slow.  Better to keep a running tab of the mean, and then multiply by number of constituent cells to get the sum, and add/subtract from that sum.  Much faster, but I'm being lazy because I'm...well..lazy?\n",
    "        :param nan:\n",
    "        :param compare: False, or h3 hex index.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if compare:\n",
    "            assert h3.h3_is_valid(compare)\n",
    "        owned_cells = self.get_owned_cells()\n",
    "        weighted_sum_list = None\n",
    "\n",
    "        # compare_flag = False\n",
    "        # compare_weighted_sum_list = None\n",
    "\n",
    "        for cell_id in owned_cells:\n",
    "            cell = self.factory.cell_dict[cell_id]\n",
    "            weight = 1 / (2 ** h3_distance(self.centroid_id, cell.hex_id))\n",
    "            component_list = tuple([x * weight for x in cell.get_data_values(True)])\n",
    "\n",
    "            # print(component_list)\n",
    "            # if compare ==  cell_id:\n",
    "            #     compare_flag = True\n",
    "            #     compare_weighted_sum = weighted_sum_list\n",
    "            # elif compare_flag:\n",
    "            #     compare_weighted_sum_list = self.pairwise_tuple_combine(compare_weighted_sum_list, component_list)\n",
    "\n",
    "            weighted_sum_list = self.pairwise_tuple_combine(weighted_sum_list, component_list)\n",
    "\n",
    "        # if compare:\n",
    "        #     counter_add = -1\n",
    "        #     if ~compare_flag:\n",
    "        #         counter_add = 1\n",
    "        #         cell = self.factory.cell_dict[cell_id]\n",
    "        #         weight = 1 / (2 ** h3_distance(self.centroid_id, cell.hex_id))\n",
    "        #         component_list = tuple([x * weight for x in cell.get_data_values(True)])\n",
    "        #         compare_weighted_sum_list = self.pairwise_tuple_combine(compare_weighted_sum_list, component_list)\n",
    "        #\n",
    "        #     mean = [x / self.cell_count for x in weighted_sum_list]\n",
    "        #     compared_mean = [x / (self.cell_count + counter_add) for x in compare_weighted_sum_list]\n",
    "        #     self.neighborhood_mean = mean\n",
    "        #     return compared_mean\n",
    "        # else:\n",
    "        #     mean = [x / self.cell_count for x in weighted_sum_list]\n",
    "        #     return mean\n",
    "        div_list = [x / self.cell_count for x in weighted_sum_list]\n",
    "\n",
    "        return self.neighborhood_mean\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def update_factory(self):\n",
    "        return self\n",
    "\n",
    "    def output_cell_map(self):\n",
    "        out = pd.DataFrame(self.get_owned_cells(), columns=['hex_id'])\n",
    "        out['name'] = self.name\n",
    "        out['centroid_id'] = self.centroid_id\n",
    "        return out\n",
    "\n",
    "    def output_data(self):\n",
    "        self.recompute_mean()\n",
    "        return [self.centroid_id, self.get_owned_cells(), self.neighborhood_mean]\n",
    "\n",
    "    @staticmethod\n",
    "    def pairwise_tuple_combine(tup_a, tup_b):\n",
    "        if tup_a == None:\n",
    "            # print(\"Warning: tup_a is 'None'\")\n",
    "            return tup_b\n",
    "        out = tuple([i + j for i, j in zip(tup_a, tup_b)])\n",
    "        return out\n",
    "\n",
    "    def surface_tension_check(self, konstant):\n",
    "        \"\"\"\n",
    "        Surface Tension = Surface Force/ edge length of force\n",
    "        :param konstant:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hood_df = pd.read_csv('../../data/clean/neighborhood_centroids.csv')\n",
    "\n",
    "hood_df['hex_id'] = hood_df['h3_10']\n",
    "hood_df = pd.DataFrame(hood_df[['hex_id', 'Name']])\n",
    "\n",
    "# hex_list = pd.read_csv('../../data/clean/h3_index/manhattan_hex.csv')\n",
    "# hex_list['hex_id'] = hex_list['manhattan_hex']\n",
    "# hex_list = pd.DataFrame(hex_list['hex_id'])\n",
    "# data_list = helper_inner.join(hex_list.set_index('hex_id'), how='inner')\n",
    "\n",
    "hex_list = wopt.reset_index()\n",
    "\n",
    "workpls = list(wopt.reset_index()['hex_id']) + list(hood_df['hex_id'])[1:]\n",
    "# hood_df = [print(row['hex_id']) for _, row in hood_df.iterrows()]\n",
    "# hood_df = [([row['hex_id'], row['Name']]) for _, row in hood_df.iterrows() if row['hex_id'].isin(list(hex_list['hex_id']))]\n",
    "workpls = list(set(workpls))\n",
    "workpls = workpls\n",
    "hex_list = pd.DataFrame(workpls, columns=['hex_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# db_scaled = pd.DataFrame(robust_scale(helper_inner, axis=1)).set_index(helper_inner.index)\n",
    "# hood_df2 = hood_df.filter(items=wopt.reset_index())\n",
    "the_filter = pd.read_csv('../../data/clean/h3_index/manhattan_hex.csv', names=['hex_id'], header=0)\n",
    "the_mask = pd.DataFrame(park_mask.index, columns=['hex_id'])\n",
    "\n",
    "\n",
    "def filter(df, filter):\n",
    "    return df[df['hex_id'].isin(filter['hex_id'])]\n",
    "\n",
    "\n",
    "def mask(df, mask):\n",
    "    return df[~df['hex_id'].isin(mask['hex_id'])]\n",
    "\n",
    "\n",
    "in_hex = pd.DataFrame(workpls, columns=['hex_id'])\n",
    "in_hex = mask(filter(in_hex, the_filter), the_mask)\n",
    "in_hex = in_hex.reset_index(drop=True)\n",
    "\n",
    "hood_df = mask(filter(hood_df, the_filter), the_mask)\n",
    "\n",
    "db_scaled = pd.DataFrame(robust_scale(all_hexes)).set_index(all_hexes.index)\n",
    "db_scaled.index.names = ['hex_id']\n",
    "db_scaled = filter(db_scaled.reset_index(), the_filter)\n",
    "db_scaled = mask(db_scaled, the_mask)\n",
    "db_scaled = db_scaled.set_index('hex_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## *I'm fixing a hole...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fixing_a_hole():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pca3 = PCA(n_components=3)\n",
    "pcps = pca3.fit_transform(db_scaled)\n",
    "# principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'PC3','PC4','PC5'])\n",
    "principalDf = pd.DataFrame(data=pcps, columns=['principal component 1', 'principal component 2', 'PC3'])\n",
    "principalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# in_hex.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(workpls, columns=['hex_id'])['hex_id'].str.len().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db_scaled.index.names = ['hex_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "the_factory = CellFactory(in_hex, db_scaled, hood_df, \"test_run_1\")\n",
    "the_factory.build_cell_dict(all_cells=True)\n",
    "the_factory.get_cell_dict()\n",
    "the_factory.generate_neighborhoods()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hood_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "the_factory.simulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hood_df.filter(items=hex_df).h3.h3_toe4_geo_boundary().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "the_factory.get_results()\n",
    "len(the_factory.cell_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = the_factory.get_results()\n",
    "df = df[~df['hex_id'].isin(list(park_mask.reset_index()['hex_id']))]\n",
    "\n",
    "df = df.set_index('hex_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_plot = df.h3.h3_to_geo_boundary()\n",
    "\n",
    "# Setup figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(10, 40))\n",
    "# Plot unique values choropleth including a legend and with no boundary lines\n",
    "df_plot.plot(column='name', categorical=True, legend=False, linewidth=0.5, ax=ax)\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CellFactory' object has no attribute 'neighborhood_list'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [75]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mthe_factory\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_results\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mlen\u001B[39m(the_factory\u001B[38;5;241m.\u001B[39mcell_dict\u001B[38;5;241m.\u001B[39mkeys())\n",
      "Input \u001B[0;32mIn [48]\u001B[0m, in \u001B[0;36mCellFactory.get_results\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_results\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     93\u001B[0m     results_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hood \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mneighborhood_list\u001B[49m:\n\u001B[1;32m     95\u001B[0m         \u001B[38;5;66;03m# print(hood.output_cell_map())\u001B[39;00m\n\u001B[1;32m     96\u001B[0m         results_list \u001B[38;5;241m=\u001B[39m results_list \u001B[38;5;241m+\u001B[39m [hood\u001B[38;5;241m.\u001B[39moutput_cell_map()]\n\u001B[1;32m     98\u001B[0m     output \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(results_list)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'CellFactory' object has no attribute 'neighborhood_list'"
     ]
    }
   ],
   "source": [
    "the_factory.get_results()\n",
    "len(the_factory.cell_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CellFactory' object has no attribute 'neighborhood_list'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [76]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mthe_factory\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_results\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m df \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;241m~\u001B[39mdf[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhex_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39misin(\u001B[38;5;28mlist\u001B[39m(park_mask\u001B[38;5;241m.\u001B[39mreset_index()[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhex_id\u001B[39m\u001B[38;5;124m'\u001B[39m]))]\n\u001B[1;32m      4\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mset_index(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhex_id\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [48]\u001B[0m, in \u001B[0;36mCellFactory.get_results\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_results\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     93\u001B[0m     results_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hood \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mneighborhood_list\u001B[49m:\n\u001B[1;32m     95\u001B[0m         \u001B[38;5;66;03m# print(hood.output_cell_map())\u001B[39;00m\n\u001B[1;32m     96\u001B[0m         results_list \u001B[38;5;241m=\u001B[39m results_list \u001B[38;5;241m+\u001B[39m [hood\u001B[38;5;241m.\u001B[39moutput_cell_map()]\n\u001B[1;32m     98\u001B[0m     output \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(results_list)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'CellFactory' object has no attribute 'neighborhood_list'"
     ]
    }
   ],
   "source": [
    "df = the_factory.get_results()\n",
    "df = df[~df['hex_id'].isin(list(park_mask.reset_index()['hex_id']))]\n",
    "\n",
    "df = df.set_index('hex_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [77]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_plot \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241m.\u001B[39mh3\u001B[38;5;241m.\u001B[39mh3_to_geo_boundary()\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Setup figure and ax\u001B[39;00m\n\u001B[1;32m      4\u001B[0m f, ax \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m1\u001B[39m, figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m40\u001B[39m))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_plot = df.h3.h3_to_geo_boundary()\n",
    "\n",
    "# Setup figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(10, 40))\n",
    "# Plot unique values choropleth including a legend and with no boundary lines\n",
    "df_plot.plot(column='name', categorical=True, legend=False, linewidth=0.5, ax=ax)\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mountain Brews v1.1",
   "language": "python",
   "name": "plz-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}